//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-29190527
// Cuda compilation tools, release 11.1, V11.1.105
// Based on LLVM 3.4svn
//

.version 7.1
.target sm_80
.address_size 64

	// .globl	Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0
// _ZZ94Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0E111T_cast_T_add_T_multiply_T_multiply_T_add_input_0_input_1_T_cast_T_less_equal_T_cast_input_4_input_11_red_shared has been demoted
// _ZZ94Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0E8red_buf1 has been demoted

.visible .entry Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0(
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_0,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_1,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_2,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_3,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_4,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_5,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_6
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<81>;
	.reg .f32 	%f<55>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<36>;
	// demoted variable
	.shared .align 4 .b8 _ZZ94Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0E111T_cast_T_add_T_multiply_T_multiply_T_add_input_0_input_1_T_cast_T_less_equal_T_cast_input_4_input_11_red_shared[32];
	// demoted variable
	.shared .align 4 .b8 _ZZ94Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0E8red_buf1[4096];

	ld.param.u64 	%rd12, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_0];
	ld.param.u64 	%rd13, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_1];
	ld.param.u64 	%rd14, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_2];
	ld.param.u64 	%rd15, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_3];
	ld.param.u64 	%rd16, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_4];
	ld.param.u64 	%rd11, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_5];
	ld.param.u64 	%rd17, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0_param_6];
	cvta.to.global.u64 	%rd18, %rd13;
	mov.u32 	%r9, %ctaid.y;
	cvt.u64.u32	%rd1, %r9;
	mul.wide.u32 	%rd19, %r9, 4096;
	mov.u32 	%r1, %tid.y;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd20, %r1, 512;
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32	%rd3, %r2;
	add.s64 	%rd21, %rd19, %rd20;
	add.s64 	%rd22, %rd21, %rd3;
	cvta.to.global.u64 	%rd23, %rd14;
	shl.b64 	%rd24, %rd22, 2;
	add.s64 	%rd4, %rd23, %rd24;
	ld.global.nc.f32 	%f2, [%rd4];
	// inline asm
	{  cvt.rn.f16.f32 %rs1, %f2;}

	// inline asm
	mov.f32 	%f3, 0f3F4CC000;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs1, %rs2;
  selp.u16 %rs3, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p2, %rs3, 0;
	selp.u32	%r8, 1, 0, %p2;
	// inline asm
	cvt.rn.f16.s32 %rs6, %r8;
	// inline asm
	cvta.to.global.u64 	%rd25, %rd17;
	shl.b64 	%rd26, %rd22, 1;
	add.s64 	%rd5, %rd25, %rd26;
	st.global.u16 	[%rd5], %rs6;
	cvta.to.global.u64 	%rd27, %rd15;
	add.s64 	%rd6, %rd27, %rd26;
	ld.global.nc.u16 	%rs8, [%rd6];
	cvta.to.global.u64 	%rd28, %rd12;
	mul.wide.u32 	%rd29, %r2, 2;
	add.s64 	%rd7, %rd28, %rd29;
	ld.global.nc.u16 	%rs9, [%rd7];
	// inline asm
	{add.f16 %rs7,%rs8,%rs9;
}
	// inline asm
	mov.f32 	%f4, 0f3FA00000;
	// inline asm
	{  cvt.rn.f16.f32 %rs10, %f4;}

	// inline asm
	// inline asm
	{mul.f16 %rs11,%rs7,%rs10;
}
	// inline asm
	// inline asm
	{mul.f16 %rs14,%rs11,%rs6;
}
	// inline asm
	add.s64 	%rd8, %rd18, %rd26;
	ld.global.nc.u16 	%rs19, [%rd8];
	// inline asm
	{add.f16 %rs17,%rs14,%rs19;
}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f5, %rs17;}

	// inline asm
	cvta.to.global.u64 	%rd30, %rd16;
	add.s64 	%rd9, %rd30, %rd24;
	st.global.f32 	[%rd9], %f5;
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, _ZZ94Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0E111T_cast_T_add_T_multiply_T_multiply_T_add_input_0_input_1_T_cast_T_less_equal_T_cast_input_4_input_11_red_shared;
	add.s32 	%r3, %r11, %r10;
	setp.ne.s32	%p3, %r2, 0;
	@%p3 bra 	BB0_2;

	mov.u32 	%r12, 0;
	st.shared.u32 	[%r3], %r12;

BB0_2:
	ld.global.nc.f32 	%f6, [%rd4+512];
	// inline asm
	{  cvt.rn.f16.f32 %rs21, %f6;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs22, %f3;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs21, %rs22;
  selp.u16 %rs23, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p4, %rs23, 0;
	selp.u32	%r13, 1, 0, %p4;
	// inline asm
	cvt.rn.f16.s32 %rs26, %r13;
	// inline asm
	st.global.u16 	[%rd5+256], %rs26;
	ld.global.nc.u16 	%rs28, [%rd6+256];
	ld.global.nc.u16 	%rs29, [%rd7+256];
	// inline asm
	{add.f16 %rs27,%rs28,%rs29;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs30, %f4;}

	// inline asm
	// inline asm
	{mul.f16 %rs31,%rs27,%rs30;
}
	// inline asm
	// inline asm
	{mul.f16 %rs34,%rs31,%rs26;
}
	// inline asm
	ld.global.nc.u16 	%rs39, [%rd8+256];
	// inline asm
	{add.f16 %rs37,%rs34,%rs39;
}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f9, %rs37;}

	// inline asm
	st.global.f32 	[%rd9+512], %f9;
	ld.global.nc.f32 	%f10, [%rd4+1024];
	// inline asm
	{  cvt.rn.f16.f32 %rs41, %f10;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs42, %f3;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs41, %rs42;
  selp.u16 %rs43, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p5, %rs43, 0;
	selp.u32	%r14, 1, 0, %p5;
	// inline asm
	cvt.rn.f16.s32 %rs46, %r14;
	// inline asm
	st.global.u16 	[%rd5+512], %rs46;
	ld.global.nc.u16 	%rs48, [%rd6+512];
	ld.global.nc.u16 	%rs49, [%rd7+512];
	// inline asm
	{add.f16 %rs47,%rs48,%rs49;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs50, %f4;}

	// inline asm
	// inline asm
	{mul.f16 %rs51,%rs47,%rs50;
}
	// inline asm
	// inline asm
	{mul.f16 %rs54,%rs51,%rs46;
}
	// inline asm
	ld.global.nc.u16 	%rs59, [%rd8+512];
	// inline asm
	{add.f16 %rs57,%rs54,%rs59;
}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f13, %rs57;}

	// inline asm
	st.global.f32 	[%rd9+1024], %f13;
	ld.global.nc.f32 	%f14, [%rd4+1536];
	// inline asm
	{  cvt.rn.f16.f32 %rs61, %f14;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs62, %f3;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs61, %rs62;
  selp.u16 %rs63, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p6, %rs63, 0;
	selp.u32	%r15, 1, 0, %p6;
	// inline asm
	cvt.rn.f16.s32 %rs66, %r15;
	// inline asm
	st.global.u16 	[%rd5+768], %rs66;
	ld.global.nc.u16 	%rs68, [%rd6+768];
	ld.global.nc.u16 	%rs69, [%rd7+768];
	// inline asm
	{add.f16 %rs67,%rs68,%rs69;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs70, %f4;}

	// inline asm
	// inline asm
	{mul.f16 %rs71,%rs67,%rs70;
}
	// inline asm
	// inline asm
	{mul.f16 %rs74,%rs71,%rs66;
}
	// inline asm
	ld.global.nc.u16 	%rs79, [%rd8+768];
	// inline asm
	{add.f16 %rs77,%rs74,%rs79;
}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f17, %rs77;}

	// inline asm
	st.global.f32 	[%rd9+1536], %f17;
	bar.sync 	0;
	ld.global.f32 	%f18, [%rd9];
	add.f32 	%f19, %f18, 0f00000000;
	sub.f32 	%f20, %f19, %f18;
	ld.global.f32 	%f21, [%rd9+512];
	sub.f32 	%f22, %f21, %f20;
	add.f32 	%f23, %f19, %f22;
	sub.f32 	%f24, %f23, %f19;
	sub.f32 	%f25, %f24, %f22;
	ld.global.f32 	%f26, [%rd9+1024];
	sub.f32 	%f27, %f26, %f25;
	add.f32 	%f28, %f23, %f27;
	sub.f32 	%f29, %f28, %f23;
	sub.f32 	%f30, %f29, %f27;
	ld.global.f32 	%f31, [%rd9+1536];
	sub.f32 	%f32, %f31, %f30;
	add.f32 	%f33, %f28, %f32;
	mov.u32 	%r16, %ntid.x;
	mad.lo.s32 	%r17, %r16, %r1, %r2;
	and.b32  	%r4, %r17, 127;
	and.b32  	%r5, %r17, -128;
	add.s32 	%r18, %r5, %r4;
	shl.b32 	%r19, %r18, 2;
	mov.u32 	%r20, _ZZ94Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_Mul_split_3453118724546123446_kernel0E8red_buf1;
	add.s32 	%r6, %r20, %r19;
	st.shared.f32 	[%r6], %f33;
	bar.sync 	0;
	setp.gt.u32	%p7, %r4, 63;
	@%p7 bra 	BB0_4;

	ld.shared.f32 	%f34, [%r6];
	ld.shared.f32 	%f35, [%r6+256];
	add.f32 	%f36, %f34, %f35;
	st.shared.f32 	[%r6], %f36;

BB0_4:
	bar.sync 	0;
	setp.gt.u32	%p8, %r4, 31;
	@%p8 bra 	BB0_6;

	ld.shared.f32 	%f37, [%r6];
	ld.shared.f32 	%f38, [%r6+128];
	add.f32 	%f39, %f37, %f38;
	st.shared.f32 	[%r6], %f39;

BB0_6:
	setp.lt.u32	%p1, %r4, 32;
	bar.sync 	0;
	@!%p1 bra 	BB0_9;
	bra.uni 	BB0_7;

BB0_7:
	ld.shared.f32 	%f40, [%r6];
	mov.b32 	 %r21, %f40;
	mov.u32 	%r22, 2;
	mov.u32 	%r23, 31;
	mov.u32 	%r24, 16;
	mov.u32 	%r25, -1;
	shfl.sync.down.b32 	%r26|%p9, %r21, %r24, %r23, %r25;
	mov.b32 	 %f41, %r26;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	 %r27, %f42;
	mov.u32 	%r28, 8;
	shfl.sync.down.b32 	%r29|%p10, %r27, %r28, %r23, %r25;
	mov.b32 	 %f43, %r29;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	 %r30, %f44;
	mov.u32 	%r31, 4;
	shfl.sync.down.b32 	%r32|%p11, %r30, %r31, %r23, %r25;
	mov.b32 	 %f45, %r32;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	 %r33, %f46;
	shfl.sync.down.b32 	%r34|%p12, %r33, %r22, %r23, %r25;
	mov.b32 	 %f47, %r34;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	 %r35, %f48;
	mov.u32 	%r36, 1;
	shfl.sync.down.b32 	%r37|%p13, %r35, %r36, %r23, %r25;
	mov.b32 	 %f49, %r37;
	add.f32 	%f1, %f48, %f49;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB0_9;

	st.shared.f32 	[%r6], %f1;

BB0_9:
	bar.sync 	0;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB0_11;

	shl.b32 	%r38, %r5, 2;
	add.s32 	%r40, %r20, %r38;
	ld.shared.f32 	%f50, [%r40];
	ld.shared.f32 	%f51, [%r3];
	add.f32 	%f52, %f51, %f50;
	st.shared.f32 	[%r3], %f52;

BB0_11:
	cvt.u32.u64	%r7, %rd3;
	bar.sync 	0;
	setp.ne.s32	%p16, %r7, 127;
	@%p16 bra 	BB0_13;

	ld.shared.f32 	%f53, [%r3];
	mul.f32 	%f54, %f53, 0f3B000000;
	shl.b64 	%rd31, %rd1, 3;
	add.s64 	%rd32, %rd31, %rd2;
	cvta.to.global.u64 	%rd33, %rd11;
	shl.b64 	%rd34, %rd32, 2;
	add.s64 	%rd35, %rd33, %rd34;
	st.global.f32 	[%rd35], %f54;

BB0_13:
	bar.sync 	0;
	ret;
}


