//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-29190527
// Cuda compilation tools, release 11.1, V11.1.105
// Based on LLVM 3.4svn
//

.version 7.1
.target sm_80
.address_size 64

	// .globl	Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0

.visible .entry Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0(
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_0,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_1,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_2,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_3,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_4,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_5,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_6,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_7,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_8,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_9,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_10,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_11,
	.param .u64 Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_12
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<81>;
	.reg .f32 	%f<217>;
	.reg .b32 	%r<138>;
	.reg .b64 	%rd<56>;


	ld.param.u64 	%rd5, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_0];
	ld.param.u64 	%rd6, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_1];
	ld.param.u64 	%rd7, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_2];
	ld.param.u64 	%rd8, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_3];
	ld.param.u64 	%rd9, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_4];
	ld.param.u64 	%rd16, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_5];
	ld.param.u64 	%rd10, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_6];
	ld.param.u64 	%rd11, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_7];
	ld.param.u64 	%rd12, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_8];
	ld.param.u64 	%rd13, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_9];
	ld.param.u64 	%rd14, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_10];
	ld.param.u64 	%rd15, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_11];
	ld.param.u64 	%rd17, [Fused_Reshape_Mul_Reshape_Add_Mul_LessEqual_Cast_Mul_Cast_Reshape_Cast_Reshap_more_parallel_14585788642886517514_kernel0_param_12];
	cvta.to.global.u64 	%rd1, %rd17;
	cvta.to.global.u64 	%rd2, %rd16;
	mov.u32 	%r1, %ctaid.x;
	setp.lt.s32	%p1, %r1, 384;
	mov.u32 	%r2, %tid.x;
	@%p1 bra 	BB0_8;
	bra.uni 	BB0_1;

BB0_8:
	cvta.to.global.u64 	%rd40, %rd10;
	shr.s32 	%r88, %r1, 31;
	shr.u32 	%r89, %r88, 28;
	add.s32 	%r90, %r1, %r89;
	shl.b32 	%r91, %r90, 12;
	and.b32  	%r92, %r91, -65536;
	shr.s32 	%r93, %r2, 31;
	shr.u32 	%r94, %r93, 24;
	add.s32 	%r95, %r2, %r94;
	shl.b32 	%r96, %r95, 6;
	and.b32  	%r97, %r96, -16384;
	add.s32 	%r98, %r97, %r92;
	and.b32  	%r99, %r90, 4194288;
	sub.s32 	%r100, %r1, %r99;
	shl.b32 	%r101, %r100, 10;
	add.s32 	%r102, %r98, %r101;
	and.b32  	%r103, %r95, 1073741568;
	sub.s32 	%r104, %r2, %r103;
	shl.b32 	%r105, %r104, 2;
	add.s32 	%r106, %r102, %r105;
	cvta.to.global.u64 	%rd41, %rd6;
	mul.wide.s32 	%rd42, %r106, 4;
	add.s64 	%rd43, %rd41, %rd42;
	ld.global.nc.v4.f32 	{%f181, %f182, %f183, %f184}, [%rd43];
	add.s32 	%r107, %r105, %r101;
	cvta.to.global.u64 	%rd44, %rd5;
	mul.wide.s32 	%rd45, %r107, 4;
	add.s64 	%rd46, %rd44, %rd45;
	ld.global.nc.v4.f32 	{%f189, %f190, %f191, %f192}, [%rd46];
	cvta.to.global.u64 	%rd47, %rd7;
	add.s64 	%rd48, %rd47, %rd42;
	ld.global.nc.v4.f32 	{%f197, %f198, %f199, %f200}, [%rd48];
	setp.le.f32	%p10, %f181, 0f3F4CCCCD;
	selp.u32	%r108, 1, 0, %p10;
	fma.rn.f32 	%f205, %f197, 0f41B504F5, %f189;
	mul.f32 	%f206, %f205, 0f3FA00000;
	selp.f32	%f173, %f206, 0f00000000, %p10;
	setp.le.f32	%p11, %f182, 0f3F4CCCCD;
	selp.u32	%r109, 1, 0, %p11;
	fma.rn.f32 	%f207, %f198, 0f41B504F5, %f190;
	mul.f32 	%f208, %f207, 0f3FA00000;
	selp.f32	%f175, %f208, 0f00000000, %p11;
	setp.le.f32	%p12, %f183, 0f3F4CCCCD;
	selp.u32	%r110, 1, 0, %p12;
	fma.rn.f32 	%f209, %f199, 0f41B504F5, %f191;
	mul.f32 	%f210, %f209, 0f3FA00000;
	selp.f32	%f177, %f210, 0f00000000, %p12;
	setp.le.f32	%p13, %f184, 0f3F4CCCCD;
	selp.u32	%r111, 1, 0, %p13;
	fma.rn.f32 	%f211, %f200, 0f41B504F5, %f192;
	mul.f32 	%f212, %f211, 0f3FA00000;
	selp.f32	%f179, %f212, 0f00000000, %p13;
	mul.hi.s32 	%r112, %r1, 715827883;
	shr.u32 	%r113, %r112, 31;
	shr.s32 	%r114, %r112, 6;
	add.s32 	%r115, %r114, %r113;
	mul.lo.s32 	%r116, %r115, 384;
	sub.s32 	%r117, %r1, %r116;
	shr.s32 	%r118, %r117, 31;
	shr.u32 	%r119, %r118, 28;
	add.s32 	%r120, %r117, %r119;
	shl.b32 	%r121, %r120, 12;
	and.b32  	%r122, %r121, -65536;
	shr.u32 	%r123, %r93, 22;
	add.s32 	%r124, %r2, %r123;
	and.b32  	%r125, %r124, -1024;
	sub.s32 	%r126, %r2, %r125;
	shr.s32 	%r127, %r126, 31;
	shr.u32 	%r128, %r127, 24;
	add.s32 	%r129, %r126, %r128;
	shl.b32 	%r130, %r129, 6;
	and.b32  	%r131, %r130, -16384;
	and.b32  	%r132, %r129, 1073741568;
	sub.s32 	%r133, %r126, %r132;
	shl.b32 	%r134, %r133, 2;
	add.s32 	%r135, %r122, %r101;
	add.s32 	%r136, %r135, %r131;
	add.s32 	%r137, %r136, %r134;
	mul.wide.s32 	%rd49, %r137, 4;
	add.s64 	%rd50, %rd40, %rd49;
	// inline asm
	{  cvt.rn.f16.f32 %rs79, %f179;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f180, %rs79;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs77, %f177;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f178, %rs77;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs75, %f175;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f176, %rs75;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs73, %f173;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f174, %rs73;}

	// inline asm
	st.global.v4.f32 	[%rd50], {%f174, %f176, %f178, %f180};
	cvta.to.global.u64 	%rd51, %rd12;
	add.s64 	%rd52, %rd51, %rd49;
	cvt.rn.f32.s32	%f213, %r111;
	cvt.rn.f32.s32	%f214, %r110;
	cvt.rn.f32.s32	%f215, %r109;
	cvt.rn.f32.s32	%f216, %r108;
	st.global.v4.f32 	[%rd52], {%f216, %f215, %f214, %f213};
	cvta.to.global.u64 	%rd53, %rd11;
	mul.wide.s32 	%rd54, %r137, 2;
	add.s64 	%rd55, %rd53, %rd54;
	st.global.v4.u16 	[%rd55], {%rs73, %rs75, %rs77, %rs79};
	bra.uni 	BB0_9;

BB0_1:
	setp.lt.s32	%p2, %r1, 768;
	@%p2 bra 	BB0_7;
	bra.uni 	BB0_2;

BB0_7:
	shr.s32 	%r23, %r1, 31;
	shr.u32 	%r24, %r23, 28;
	add.s32 	%r25, %r1, %r24;
	and.b32  	%r26, %r25, 4194288;
	sub.s32 	%r27, %r1, %r26;
	shl.b32 	%r28, %r27, 10;
	shr.s32 	%r29, %r2, 31;
	shr.u32 	%r30, %r29, 24;
	add.s32 	%r31, %r2, %r30;
	and.b32  	%r32, %r31, 1073741568;
	sub.s32 	%r33, %r2, %r32;
	shl.b32 	%r34, %r33, 2;
	add.s32 	%r35, %r34, %r28;
	cvta.to.global.u64 	%rd24, %rd5;
	mul.wide.s32 	%rd25, %r35, 4;
	add.s64 	%rd26, %rd24, %rd25;
	ld.global.nc.v4.f32 	{%f137, %f138, %f139, %f140}, [%rd26];
	mul.hi.s32 	%r36, %r1, 715827883;
	shr.u32 	%r37, %r36, 31;
	shr.s32 	%r38, %r36, 6;
	add.s32 	%r39, %r38, %r37;
	mul.lo.s32 	%r40, %r39, 384;
	sub.s32 	%r41, %r1, %r40;
	shr.s32 	%r42, %r41, 31;
	shr.u32 	%r43, %r42, 28;
	add.s32 	%r44, %r41, %r43;
	shl.b32 	%r45, %r44, 12;
	and.b32  	%r46, %r45, -65536;
	shl.b32 	%r47, %r31, 6;
	and.b32  	%r48, %r47, -16384;
	add.s32 	%r49, %r35, %r46;
	add.s32 	%r50, %r49, %r48;
	cvta.to.global.u64 	%rd27, %rd8;
	mul.wide.s32 	%rd28, %r50, 4;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.v4.f32 	{%f145, %f146, %f147, %f148}, [%rd29];
	cvta.to.global.u64 	%rd30, %rd9;
	add.s64 	%rd31, %rd30, %rd28;
	ld.global.nc.v4.f32 	{%f153, %f154, %f155, %f156}, [%rd31];
	setp.le.f32	%p6, %f145, 0f3F4CCCCD;
	selp.u32	%r51, 1, 0, %p6;
	fma.rn.f32 	%f161, %f153, 0f41B504F5, %f137;
	mul.f32 	%f162, %f161, 0f3FA00000;
	selp.f32	%f129, %f162, 0f00000000, %p6;
	setp.le.f32	%p7, %f146, 0f3F4CCCCD;
	selp.u32	%r52, 1, 0, %p7;
	fma.rn.f32 	%f163, %f154, 0f41B504F5, %f138;
	mul.f32 	%f164, %f163, 0f3FA00000;
	selp.f32	%f131, %f164, 0f00000000, %p7;
	setp.le.f32	%p8, %f147, 0f3F4CCCCD;
	selp.u32	%r53, 1, 0, %p8;
	fma.rn.f32 	%f165, %f155, 0f41B504F5, %f139;
	mul.f32 	%f166, %f165, 0f3FA00000;
	selp.f32	%f133, %f166, 0f00000000, %p8;
	setp.le.f32	%p9, %f148, 0f3F4CCCCD;
	selp.u32	%r54, 1, 0, %p9;
	fma.rn.f32 	%f167, %f156, 0f41B504F5, %f140;
	mul.f32 	%f168, %f167, 0f3FA00000;
	selp.f32	%f135, %f168, 0f00000000, %p9;
	add.s32 	%r55, %r1, -384;
	mul.hi.s32 	%r56, %r55, 715827883;
	shr.u32 	%r57, %r56, 31;
	shr.s32 	%r58, %r56, 6;
	add.s32 	%r59, %r58, %r57;
	mul.lo.s32 	%r60, %r59, 384;
	sub.s32 	%r61, %r55, %r60;
	shr.s32 	%r62, %r61, 31;
	shr.u32 	%r63, %r62, 28;
	add.s32 	%r64, %r61, %r63;
	shl.b32 	%r65, %r64, 12;
	and.b32  	%r66, %r65, -65536;
	shr.u32 	%r67, %r29, 22;
	add.s32 	%r68, %r2, %r67;
	and.b32  	%r69, %r68, -1024;
	sub.s32 	%r70, %r2, %r69;
	shr.s32 	%r71, %r70, 31;
	shr.u32 	%r72, %r71, 24;
	add.s32 	%r73, %r70, %r72;
	shl.b32 	%r74, %r73, 6;
	and.b32  	%r75, %r74, -16384;
	add.s32 	%r76, %r75, %r66;
	shr.s32 	%r77, %r55, 31;
	shr.u32 	%r78, %r77, 28;
	add.s32 	%r79, %r55, %r78;
	and.b32  	%r80, %r79, 4194288;
	sub.s32 	%r81, %r55, %r80;
	shl.b32 	%r82, %r81, 10;
	add.s32 	%r83, %r76, %r82;
	and.b32  	%r84, %r73, 1073741568;
	sub.s32 	%r85, %r70, %r84;
	shl.b32 	%r86, %r85, 2;
	add.s32 	%r87, %r83, %r86;
	cvta.to.global.u64 	%rd32, %rd13;
	mul.wide.s32 	%rd33, %r87, 4;
	add.s64 	%rd34, %rd32, %rd33;
	// inline asm
	{  cvt.rn.f16.f32 %rs71, %f135;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f136, %rs71;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs69, %f133;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f134, %rs69;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs67, %f131;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f132, %rs67;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs65, %f129;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f130, %rs65;}

	// inline asm
	st.global.v4.f32 	[%rd34], {%f130, %f132, %f134, %f136};
	cvta.to.global.u64 	%rd35, %rd15;
	add.s64 	%rd36, %rd35, %rd33;
	cvt.rn.f32.s32	%f169, %r54;
	cvt.rn.f32.s32	%f170, %r53;
	cvt.rn.f32.s32	%f171, %r52;
	cvt.rn.f32.s32	%f172, %r51;
	st.global.v4.f32 	[%rd36], {%f172, %f171, %f170, %f169};
	cvta.to.global.u64 	%rd37, %rd14;
	mul.wide.s32 	%rd38, %r87, 2;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.v4.u16 	[%rd39], {%rs65, %rs67, %rs69, %rs71};
	bra.uni 	BB0_9;

BB0_2:
	setp.gt.s32	%p3, %r2, 511;
	@%p3 bra 	BB0_9;

	setp.lt.s32	%p4, %r1, 2057;
	shl.b32 	%r3, %r2, 2;
	shr.s32 	%r5, %r2, 31;
	shr.u32 	%r6, %r5, 23;
	add.s32 	%r7, %r2, %r6;
	and.b32  	%r8, %r7, 1073741312;
	sub.s32 	%r9, %r2, %r8;
	shl.b32 	%r4, %r9, 2;
	@%p4 bra 	BB0_6;
	bra.uni 	BB0_4;

BB0_6:
	add.s32 	%r11, %r1, -768;
	mul.hi.s32 	%r12, %r11, 1704669191;
	shr.u32 	%r13, %r12, 31;
	shr.u32 	%r14, %r12, 9;
	add.s32 	%r15, %r14, %r13;
	mul.lo.s32 	%r16, %r15, 1290;
	sub.s32 	%r17, %r11, %r16;
	shl.b32 	%r18, %r17, 14;
	shl.b32 	%r19, %r1, 14;
	add.s32 	%r20, %r19, %r3;
	add.s32 	%r21, %r20, -12582912;
	mul.wide.s32 	%rd20, %r21, 4;
	add.s64 	%rd21, %rd2, %rd20;
	ld.global.nc.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd21];
	add.s32 	%r22, %r18, %r4;
	mul.wide.s32 	%rd22, %r22, 2;
	add.s64 	%rd23, %rd1, %rd22;
	// inline asm
	{  cvt.rn.f16.f32 %rs36, %f100;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs35, %f99;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs34, %f98;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs33, %f97;}

	// inline asm
	st.global.v4.u16 	[%rd23], {%rs33, %rs34, %rs35, %rs36};
	ld.global.nc.v4.f32 	{%f101, %f102, %f103, %f104}, [%rd21+8192];
	// inline asm
	{  cvt.rn.f16.f32 %rs40, %f104;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs39, %f103;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs38, %f102;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs37, %f101;}

	// inline asm
	st.global.v4.u16 	[%rd23+4096], {%rs37, %rs38, %rs39, %rs40};
	ld.global.nc.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd21+16384];
	// inline asm
	{  cvt.rn.f16.f32 %rs44, %f108;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs43, %f107;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs42, %f106;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs41, %f105;}

	// inline asm
	st.global.v4.u16 	[%rd23+8192], {%rs41, %rs42, %rs43, %rs44};
	ld.global.nc.v4.f32 	{%f109, %f110, %f111, %f112}, [%rd21+24576];
	// inline asm
	{  cvt.rn.f16.f32 %rs48, %f112;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs47, %f111;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs46, %f110;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs45, %f109;}

	// inline asm
	st.global.v4.u16 	[%rd23+12288], {%rs45, %rs46, %rs47, %rs48};
	ld.global.nc.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd21+32768];
	// inline asm
	{  cvt.rn.f16.f32 %rs52, %f116;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs51, %f115;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs50, %f114;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs49, %f113;}

	// inline asm
	st.global.v4.u16 	[%rd23+16384], {%rs49, %rs50, %rs51, %rs52};
	ld.global.nc.v4.f32 	{%f117, %f118, %f119, %f120}, [%rd21+40960];
	// inline asm
	{  cvt.rn.f16.f32 %rs56, %f120;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs55, %f119;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs54, %f118;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs53, %f117;}

	// inline asm
	st.global.v4.u16 	[%rd23+20480], {%rs53, %rs54, %rs55, %rs56};
	ld.global.nc.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd21+49152];
	// inline asm
	{  cvt.rn.f16.f32 %rs60, %f124;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs59, %f123;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs58, %f122;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs57, %f121;}

	// inline asm
	st.global.v4.u16 	[%rd23+24576], {%rs57, %rs58, %rs59, %rs60};
	ld.global.nc.v4.f32 	{%f125, %f126, %f127, %f128}, [%rd21+57344];
	// inline asm
	{  cvt.rn.f16.f32 %rs64, %f128;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs63, %f127;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs62, %f126;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs61, %f125;}

	// inline asm
	st.global.v4.u16 	[%rd23+28672], {%rs61, %rs62, %rs63, %rs64};
	bra.uni 	BB0_9;

BB0_4:
	mul.wide.s32 	%rd18, %r3, 4;
	add.s64 	%rd3, %rd2, %rd18;
	ld.global.nc.v4.f32 	{%f29, %f30, %f31, %f32}, [%rd3+84475904];
	add.s32 	%r10, %r4, 21118976;
	mul.wide.s32 	%rd19, %r10, 2;
	add.s64 	%rd4, %rd1, %rd19;
	// inline asm
	{  cvt.rn.f16.f32 %rs4, %f32;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs3, %f31;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f30;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs1, %f29;}

	// inline asm
	st.global.v4.u16 	[%rd4], {%rs1, %rs2, %rs3, %rs4};
	ld.global.nc.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd3+84484096];
	// inline asm
	{  cvt.rn.f16.f32 %rs8, %f36;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs7, %f35;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs6, %f34;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs5, %f33;}

	// inline asm
	st.global.v4.u16 	[%rd4+4096], {%rs5, %rs6, %rs7, %rs8};
	ld.global.nc.v4.f32 	{%f37, %f38, %f39, %f40}, [%rd3+84492288];
	// inline asm
	{  cvt.rn.f16.f32 %rs12, %f40;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs11, %f39;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs10, %f38;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs9, %f37;}

	// inline asm
	st.global.v4.u16 	[%rd4+8192], {%rs9, %rs10, %rs11, %rs12};
	ld.global.nc.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd3+84500480];
	// inline asm
	{  cvt.rn.f16.f32 %rs16, %f44;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs15, %f43;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs14, %f42;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs13, %f41;}

	// inline asm
	st.global.v4.u16 	[%rd4+12288], {%rs13, %rs14, %rs15, %rs16};
	ld.global.nc.v4.f32 	{%f45, %f46, %f47, %f48}, [%rd3+84508672];
	// inline asm
	{  cvt.rn.f16.f32 %rs20, %f48;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs19, %f47;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs18, %f46;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs17, %f45;}

	// inline asm
	st.global.v4.u16 	[%rd4+16384], {%rs17, %rs18, %rs19, %rs20};
	ld.global.nc.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd3+84516864];
	// inline asm
	{  cvt.rn.f16.f32 %rs24, %f52;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs23, %f51;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs22, %f50;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs21, %f49;}

	// inline asm
	st.global.v4.u16 	[%rd4+20480], {%rs21, %rs22, %rs23, %rs24};
	ld.global.nc.v4.f32 	{%f53, %f54, %f55, %f56}, [%rd3+84525056];
	// inline asm
	{  cvt.rn.f16.f32 %rs28, %f56;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs27, %f55;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs26, %f54;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs25, %f53;}

	// inline asm
	st.global.v4.u16 	[%rd4+24576], {%rs25, %rs26, %rs27, %rs28};
	setp.gt.s32	%p5, %r2, 383;
	@%p5 bra 	BB0_9;

	ld.global.nc.v4.f32 	{%f61, %f62, %f63, %f64}, [%rd3+84533248];
	// inline asm
	{  cvt.rn.f16.f32 %rs32, %f64;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs31, %f63;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs30, %f62;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs29, %f61;}

	// inline asm
	st.global.v4.u16 	[%rd4+28672], {%rs29, %rs30, %rs31, %rs32};

BB0_9:
	ret;
}


