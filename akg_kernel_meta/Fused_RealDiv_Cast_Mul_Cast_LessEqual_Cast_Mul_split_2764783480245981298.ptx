//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-29190527
// Cuda compilation tools, release 11.1, V11.1.105
// Based on LLVM 3.4svn
//

.version 7.1
.target sm_80
.address_size 64

	// .globl	Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0
// _ZZ80Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0E14input_1_shared has been demoted

.visible .entry Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0(
	.param .u64 Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_0,
	.param .u64 Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_1,
	.param .u64 Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_2,
	.param .u64 Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_3,
	.param .u64 Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_4,
	.param .u64 Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_5
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<113>;
	.reg .f32 	%f<60>;
	.reg .b32 	%r<27>;
	.reg .b64 	%rd<22>;
	// demoted variable
	.shared .align 4 .b8 _ZZ80Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0E14input_1_shared[128];

	ld.param.u64 	%rd1, [Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_0];
	ld.param.u64 	%rd2, [Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_1];
	ld.param.u64 	%rd3, [Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_2];
	ld.param.u64 	%rd4, [Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_3];
	ld.param.u64 	%rd5, [Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_4];
	ld.param.u64 	%rd6, [Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0_param_5];
	mov.u32 	%r1, %tid.x;
	setp.gt.s32	%p1, %r1, 31;
	@%p1 bra 	BB0_2;

	mov.u32 	%r3, %ctaid.x;
	shl.b32 	%r4, %r3, 5;
	add.s32 	%r5, %r4, %r1;
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r5, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.f32 	%f1, [%rd9];
	shl.b32 	%r6, %r1, 2;
	mov.u32 	%r7, _ZZ80Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0E14input_1_shared;
	add.s32 	%r8, %r7, %r6;
	st.shared.f32 	[%r8], %f1;

BB0_2:
	mov.u32 	%r2, %ctaid.x;
	bar.sync 	0;
	shl.b32 	%r17, %r2, 13;
	shl.b32 	%r18, %r1, 2;
	add.s32 	%r19, %r18, %r17;
	cvta.to.global.u64 	%rd10, %rd3;
	mul.wide.s32 	%rd11, %r19, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.v4.f32 	{%f34, %f35, %f36, %f37}, [%rd12];
	cvta.to.global.u64 	%rd13, %rd1;
	add.s64 	%rd14, %rd13, %rd11;
	ld.global.nc.v4.f32 	{%f38, %f39, %f40, %f41}, [%rd14];
	shr.s32 	%r20, %r1, 31;
	shr.u32 	%r21, %r20, 26;
	add.s32 	%r22, %r1, %r21;
	shr.s32 	%r23, %r22, 6;
	shl.b32 	%r24, %r23, 2;
	mov.u32 	%r25, _ZZ80Fused_RealDiv_Cast_Mul_Cast_LessEqual_Cast_Mul_split_2764783480245981298_kernel0E14input_1_shared;
	add.s32 	%r26, %r25, %r24;
	ld.shared.f32 	%f46, [%r26];
	// inline asm
	{  cvt.rn.f16.f32 %rs1, %f34;}

	// inline asm
	mov.f32 	%f31, 0f3F4CC000;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f31;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs1, %rs2;
  selp.u16 %rs3, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p2, %rs3, 0;
	selp.u32	%r9, 1, 0, %p2;
	div.rn.f32 	%f4, %f38, %f46;
	// inline asm
	{  cvt.rn.f16.f32 %rs7, %f4;}

	// inline asm
	mov.f32 	%f33, 0f3FA00000;
	// inline asm
	{  cvt.rn.f16.f32 %rs8, %f33;}

	// inline asm
	// inline asm
	{mul.f16 %rs9,%rs7,%rs8;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs15, %f35;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs16, %f31;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs15, %rs16;
  selp.u16 %rs17, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p3, %rs17, 0;
	selp.u32	%r10, 1, 0, %p3;
	div.rn.f32 	%f8, %f39, %f46;
	// inline asm
	{  cvt.rn.f16.f32 %rs21, %f8;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs22, %f33;}

	// inline asm
	// inline asm
	{mul.f16 %rs23,%rs21,%rs22;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs29, %f36;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs30, %f31;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs29, %rs30;
  selp.u16 %rs31, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p4, %rs31, 0;
	selp.u32	%r11, 1, 0, %p4;
	div.rn.f32 	%f12, %f40, %f46;
	// inline asm
	{  cvt.rn.f16.f32 %rs35, %f12;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs36, %f33;}

	// inline asm
	// inline asm
	{mul.f16 %rs37,%rs35,%rs36;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs43, %f37;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs44, %f31;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs43, %rs44;
  selp.u16 %rs45, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p5, %rs45, 0;
	selp.u32	%r12, 1, 0, %p5;
	div.rn.f32 	%f16, %f41, %f46;
	// inline asm
	{  cvt.rn.f16.f32 %rs49, %f16;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs50, %f33;}

	// inline asm
	// inline asm
	{mul.f16 %rs51,%rs49,%rs50;
}
	// inline asm
	cvta.to.global.u64 	%rd15, %rd6;
	mul.wide.s32 	%rd16, %r19, 2;
	add.s64 	%rd17, %rd15, %rd16;
	// inline asm
	cvt.rn.f16.s32 %rs48, %r12;
	// inline asm
	// inline asm
	cvt.rn.f16.s32 %rs34, %r11;
	// inline asm
	// inline asm
	cvt.rn.f16.s32 %rs20, %r10;
	// inline asm
	// inline asm
	cvt.rn.f16.s32 %rs6, %r9;
	// inline asm
	st.global.v4.u16 	[%rd17], {%rs6, %rs20, %rs34, %rs48};
	cvta.to.global.u64 	%rd18, %rd4;
	add.s64 	%rd19, %rd18, %rd11;
	st.global.v4.f32 	[%rd19], {%f4, %f8, %f12, %f16};
	cvta.to.global.u64 	%rd20, %rd5;
	add.s64 	%rd21, %rd20, %rd16;
	// inline asm
	{mul.f16 %rs54,%rs51,%rs48;
}
	// inline asm
	// inline asm
	{mul.f16 %rs40,%rs37,%rs34;
}
	// inline asm
	// inline asm
	{mul.f16 %rs26,%rs23,%rs20;
}
	// inline asm
	// inline asm
	{mul.f16 %rs12,%rs9,%rs6;
}
	// inline asm
	st.global.v4.u16 	[%rd21], {%rs12, %rs26, %rs40, %rs54};
	ld.global.nc.v4.f32 	{%f47, %f48, %f49, %f50}, [%rd12+16384];
	ld.global.nc.v4.f32 	{%f51, %f52, %f53, %f54}, [%rd14+16384];
	ld.shared.f32 	%f59, [%r26+64];
	// inline asm
	{  cvt.rn.f16.f32 %rs57, %f47;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs58, %f31;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs57, %rs58;
  selp.u16 %rs59, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p6, %rs59, 0;
	selp.u32	%r13, 1, 0, %p6;
	div.rn.f32 	%f20, %f51, %f59;
	// inline asm
	{  cvt.rn.f16.f32 %rs63, %f20;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs64, %f33;}

	// inline asm
	// inline asm
	{mul.f16 %rs65,%rs63,%rs64;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs71, %f48;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs72, %f31;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs71, %rs72;
  selp.u16 %rs73, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p7, %rs73, 0;
	selp.u32	%r14, 1, 0, %p7;
	div.rn.f32 	%f24, %f52, %f59;
	// inline asm
	{  cvt.rn.f16.f32 %rs77, %f24;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs78, %f33;}

	// inline asm
	// inline asm
	{mul.f16 %rs79,%rs77,%rs78;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs85, %f49;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs86, %f31;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs85, %rs86;
  selp.u16 %rs87, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p8, %rs87, 0;
	selp.u32	%r15, 1, 0, %p8;
	div.rn.f32 	%f28, %f53, %f59;
	// inline asm
	{  cvt.rn.f16.f32 %rs91, %f28;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs92, %f33;}

	// inline asm
	// inline asm
	{mul.f16 %rs93,%rs91,%rs92;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs99, %f50;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs100, %f31;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs99, %rs100;
  selp.u16 %rs101, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p9, %rs101, 0;
	selp.u32	%r16, 1, 0, %p9;
	div.rn.f32 	%f32, %f54, %f59;
	// inline asm
	{  cvt.rn.f16.f32 %rs105, %f32;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs106, %f33;}

	// inline asm
	// inline asm
	{mul.f16 %rs107,%rs105,%rs106;
}
	// inline asm
	// inline asm
	cvt.rn.f16.s32 %rs104, %r16;
	// inline asm
	// inline asm
	cvt.rn.f16.s32 %rs90, %r15;
	// inline asm
	// inline asm
	cvt.rn.f16.s32 %rs76, %r14;
	// inline asm
	// inline asm
	cvt.rn.f16.s32 %rs62, %r13;
	// inline asm
	st.global.v4.u16 	[%rd17+8192], {%rs62, %rs76, %rs90, %rs104};
	st.global.v4.f32 	[%rd19+16384], {%f20, %f24, %f28, %f32};
	// inline asm
	{mul.f16 %rs110,%rs107,%rs104;
}
	// inline asm
	// inline asm
	{mul.f16 %rs96,%rs93,%rs90;
}
	// inline asm
	// inline asm
	{mul.f16 %rs82,%rs79,%rs76;
}
	// inline asm
	// inline asm
	{mul.f16 %rs68,%rs65,%rs62;
}
	// inline asm
	st.global.v4.u16 	[%rd21+8192], {%rs68, %rs82, %rs96, %rs110};
	bar.sync 	0;
	ret;
}


