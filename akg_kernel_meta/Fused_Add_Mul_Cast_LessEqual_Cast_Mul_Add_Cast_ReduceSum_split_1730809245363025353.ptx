//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-29190527
// Cuda compilation tools, release 11.1, V11.1.105
// Based on LLVM 3.4svn
//

.version 7.1
.target sm_80
.address_size 64

	// .globl	Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0
// _ZZ90Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0E111T_cast_T_add_T_multiply_T_multiply_T_add_input_0_input_1_T_cast_T_less_equal_T_cast_input_4_input_11_red_shared has been demoted
// _ZZ90Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0E8red_buf0 has been demoted

.visible .entry Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0(
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_0,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_1,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_2,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_3,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_4,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_5,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_6,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_7,
	.param .u64 Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_8
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<41>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<58>;
	.reg .b64 	%rd<31>;
	// demoted variable
	.shared .align 4 .b8 _ZZ90Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0E111T_cast_T_add_T_multiply_T_multiply_T_add_input_0_input_1_T_cast_T_less_equal_T_cast_input_4_input_11_red_shared[32];
	// demoted variable
	.shared .align 4 .b8 _ZZ90Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0E8red_buf0[4096];

	ld.param.u64 	%rd16, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_0];
	ld.param.u64 	%rd17, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_1];
	ld.param.u64 	%rd18, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_2];
	ld.param.u64 	%rd19, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_3];
	ld.param.u64 	%rd15, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_4];
	ld.param.u64 	%rd20, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_6];
	ld.param.u64 	%rd21, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_7];
	ld.param.u64 	%rd22, [Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0_param_8];
	cvta.to.global.u64 	%rd1, %rd21;
	cvta.to.global.u64 	%rd2, %rd17;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd19;
	cvta.to.global.u64 	%rd5, %rd22;
	cvta.to.global.u64 	%rd6, %rd18;
	cvta.to.global.u64 	%rd7, %rd20;
	mov.u32 	%r1, %tid.y;
	shl.b32 	%r2, %r1, 10;
	shl.b32 	%r18, %r1, 2;
	mov.u32 	%r19, _ZZ90Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0E111T_cast_T_add_T_multiply_T_multiply_T_add_input_0_input_1_T_cast_T_less_equal_T_cast_input_4_input_11_red_shared;
	add.s32 	%r3, %r19, %r18;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r6, %r4, 8192, %r5;
	mad.lo.s32 	%r56, %r1, 1024, %r6;
	mov.u32 	%r57, 0;
	mov.u32 	%r55, %r5;

BB0_1:
	mul.wide.s32 	%rd23, %r56, 4;
	add.s64 	%rd8, %rd6, %rd23;
	ld.global.nc.f32 	%f2, [%rd8];
	// inline asm
	{  cvt.rn.f16.f32 %rs1, %f2;}

	// inline asm
	mov.f32 	%f3, 0f3F4CC000;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs1, %rs2;
  selp.u16 %rs3, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p2, %rs3, 0;
	selp.u32	%r20, 1, 0, %p2;
	// inline asm
	cvt.rn.f16.s32 %rs6, %r20;
	// inline asm
	mul.wide.s32 	%rd24, %r56, 2;
	add.s64 	%rd9, %rd5, %rd24;
	st.global.u16 	[%rd9], %rs6;
	add.s64 	%rd10, %rd4, %rd24;
	ld.global.nc.u16 	%rs8, [%rd10];
	mul.wide.s32 	%rd25, %r55, 2;
	add.s64 	%rd11, %rd3, %rd25;
	ld.global.nc.u16 	%rs9, [%rd11];
	// inline asm
	{add.f16 %rs7,%rs8,%rs9;
}
	// inline asm
	mov.f32 	%f4, 0f3FA00000;
	// inline asm
	{  cvt.rn.f16.f32 %rs10, %f4;}

	// inline asm
	// inline asm
	{mul.f16 %rs11,%rs7,%rs10;
}
	// inline asm
	// inline asm
	{mul.f16 %rs14,%rs11,%rs6;
}
	// inline asm
	add.s64 	%rd12, %rd2, %rd24;
	ld.global.nc.u16 	%rs19, [%rd12];
	// inline asm
	{add.f16 %rs17,%rs14,%rs19;
}
	// inline asm
	add.s64 	%rd13, %rd1, %rd24;
	st.global.u16 	[%rd13], %rs17;
	// inline asm
	{  cvt.f32.f16 %f5, %rs17;}

	// inline asm
	add.s64 	%rd14, %rd7, %rd23;
	st.global.f32 	[%rd14], %f5;
	or.b32  	%r21, %r5, %r57;
	setp.ne.s32	%p3, %r21, 0;
	@%p3 bra 	BB0_3;

	mov.u32 	%r22, 0;
	st.shared.u32 	[%r3], %r22;

BB0_3:
	ld.global.nc.f32 	%f6, [%rd8+512];
	// inline asm
	{  cvt.rn.f16.f32 %rs21, %f6;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs22, %f3;}

	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.le.f16  __$temp3, %rs21, %rs22;
  selp.u16 %rs23, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p4, %rs23, 0;
	selp.u32	%r23, 1, 0, %p4;
	// inline asm
	cvt.rn.f16.s32 %rs26, %r23;
	// inline asm
	st.global.u16 	[%rd9+256], %rs26;
	ld.global.nc.u16 	%rs28, [%rd10+256];
	ld.global.nc.u16 	%rs29, [%rd11+256];
	// inline asm
	{add.f16 %rs27,%rs28,%rs29;
}
	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs30, %f4;}

	// inline asm
	// inline asm
	{mul.f16 %rs31,%rs27,%rs30;
}
	// inline asm
	// inline asm
	{mul.f16 %rs34,%rs31,%rs26;
}
	// inline asm
	ld.global.nc.u16 	%rs39, [%rd12+256];
	// inline asm
	{add.f16 %rs37,%rs34,%rs39;
}
	// inline asm
	st.global.u16 	[%rd13+256], %rs37;
	// inline asm
	{  cvt.f32.f16 %f9, %rs37;}

	// inline asm
	st.global.f32 	[%rd14+512], %f9;
	add.s32 	%r57, %r57, 2;
	add.s32 	%r56, %r56, 256;
	add.s32 	%r55, %r55, 256;
	setp.ne.s32	%p5, %r57, 8;
	@%p5 bra 	BB0_1;

	bar.sync 	0;
	add.s32 	%r24, %r6, %r2;
	mul.wide.s32 	%rd26, %r24, 4;
	add.s64 	%rd27, %rd7, %rd26;
	ld.global.f32 	%f10, [%rd27];
	add.f32 	%f11, %f10, 0f00000000;
	sub.f32 	%f12, %f11, %f10;
	ld.global.f32 	%f13, [%rd27+512];
	sub.f32 	%f14, %f13, %f12;
	add.f32 	%f15, %f11, %f14;
	sub.f32 	%f16, %f15, %f11;
	sub.f32 	%f17, %f16, %f14;
	ld.global.f32 	%f18, [%rd27+1024];
	sub.f32 	%f19, %f18, %f17;
	add.f32 	%f20, %f15, %f19;
	sub.f32 	%f21, %f20, %f15;
	sub.f32 	%f22, %f21, %f19;
	ld.global.f32 	%f23, [%rd27+1536];
	sub.f32 	%f24, %f23, %f22;
	add.f32 	%f25, %f20, %f24;
	sub.f32 	%f26, %f25, %f20;
	sub.f32 	%f27, %f26, %f24;
	ld.global.f32 	%f28, [%rd27+2048];
	sub.f32 	%f29, %f28, %f27;
	add.f32 	%f30, %f25, %f29;
	sub.f32 	%f31, %f30, %f25;
	sub.f32 	%f32, %f31, %f29;
	ld.global.f32 	%f33, [%rd27+2560];
	sub.f32 	%f34, %f33, %f32;
	add.f32 	%f35, %f30, %f34;
	sub.f32 	%f36, %f35, %f30;
	sub.f32 	%f37, %f36, %f34;
	ld.global.f32 	%f38, [%rd27+3072];
	sub.f32 	%f39, %f38, %f37;
	add.f32 	%f40, %f35, %f39;
	sub.f32 	%f41, %f40, %f35;
	sub.f32 	%f42, %f41, %f39;
	ld.global.f32 	%f43, [%rd27+3584];
	sub.f32 	%f44, %f43, %f42;
	add.f32 	%f45, %f40, %f44;
	mov.u32 	%r25, %ntid.x;
	mad.lo.s32 	%r26, %r25, %r1, %r5;
	and.b32  	%r14, %r26, 127;
	and.b32  	%r15, %r26, -128;
	add.s32 	%r27, %r15, %r14;
	shl.b32 	%r28, %r27, 2;
	mov.u32 	%r29, _ZZ90Fused_Add_Mul_Cast_LessEqual_Cast_Mul_Add_Cast_ReduceSum_split_1730809245363025353_kernel0E8red_buf0;
	add.s32 	%r16, %r29, %r28;
	st.shared.f32 	[%r16], %f45;
	bar.sync 	0;
	setp.gt.u32	%p6, %r14, 63;
	@%p6 bra 	BB0_6;

	ld.shared.f32 	%f46, [%r16];
	ld.shared.f32 	%f47, [%r16+256];
	add.f32 	%f48, %f46, %f47;
	st.shared.f32 	[%r16], %f48;

BB0_6:
	bar.sync 	0;
	setp.gt.u32	%p7, %r14, 31;
	@%p7 bra 	BB0_8;

	ld.shared.f32 	%f49, [%r16];
	ld.shared.f32 	%f50, [%r16+128];
	add.f32 	%f51, %f49, %f50;
	st.shared.f32 	[%r16], %f51;

BB0_8:
	setp.lt.u32	%p1, %r14, 32;
	bar.sync 	0;
	@!%p1 bra 	BB0_11;
	bra.uni 	BB0_9;

BB0_9:
	ld.shared.f32 	%f52, [%r16];
	mov.b32 	 %r30, %f52;
	mov.u32 	%r31, 2;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.down.b32 	%r35|%p8, %r30, %r33, %r32, %r34;
	mov.b32 	 %f53, %r35;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	 %r36, %f54;
	mov.u32 	%r37, 8;
	shfl.sync.down.b32 	%r38|%p9, %r36, %r37, %r32, %r34;
	mov.b32 	 %f55, %r38;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	 %r39, %f56;
	mov.u32 	%r40, 4;
	shfl.sync.down.b32 	%r41|%p10, %r39, %r40, %r32, %r34;
	mov.b32 	 %f57, %r41;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	 %r42, %f58;
	shfl.sync.down.b32 	%r43|%p11, %r42, %r31, %r32, %r34;
	mov.b32 	 %f59, %r43;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	 %r44, %f60;
	mov.u32 	%r45, 1;
	shfl.sync.down.b32 	%r46|%p12, %r44, %r45, %r32, %r34;
	mov.b32 	 %f61, %r46;
	add.f32 	%f1, %f60, %f61;
	setp.ne.s32	%p13, %r14, 0;
	@%p13 bra 	BB0_11;

	st.shared.f32 	[%r16], %f1;

BB0_11:
	bar.sync 	0;
	setp.ne.s32	%p14, %r14, 0;
	@%p14 bra 	BB0_13;

	shl.b32 	%r47, %r15, 2;
	add.s32 	%r49, %r29, %r47;
	ld.shared.f32 	%f62, [%r49];
	ld.shared.f32 	%f63, [%r3];
	add.f32 	%f64, %f63, %f62;
	st.shared.f32 	[%r3], %f64;

BB0_13:
	bar.sync 	0;
	setp.eq.s32	%p15, %r1, 0;
	setp.lt.s32	%p16, %r5, 8;
	and.pred  	%p17, %p16, %p15;
	@!%p17 bra 	BB0_15;
	bra.uni 	BB0_14;

BB0_14:
	shl.b32 	%r50, %r5, 2;
	add.s32 	%r52, %r19, %r50;
	ld.shared.f32 	%f65, [%r52];
	shl.b32 	%r53, %r4, 3;
	add.s32 	%r54, %r53, %r5;
	cvta.to.global.u64 	%rd28, %rd15;
	mul.wide.s32 	%rd29, %r54, 4;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.f32 	[%rd30], %f65;

BB0_15:
	bar.sync 	0;
	ret;
}


