//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-29190527
// Cuda compilation tools, release 11.1, V11.1.105
// Based on LLVM 3.4svn
//

.version 7.1
.target sm_80
.address_size 64

	// .globl	Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0
// _ZZ67Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0E53T_cast_T_add_T_add_input_0_input_1_input_3_red_shared has been demoted
// _ZZ67Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0E8red_buf1 has been demoted

.visible .entry Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0(
	.param .u64 Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_0,
	.param .u64 Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_1,
	.param .u64 Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_2,
	.param .u64 Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_3,
	.param .u64 Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_4,
	.param .u64 Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_5
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<29>;
	.reg .f32 	%f<43>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<33>;
	// demoted variable
	.shared .align 4 .b8 _ZZ67Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0E53T_cast_T_add_T_add_input_0_input_1_input_3_red_shared[32];
	// demoted variable
	.shared .align 4 .b8 _ZZ67Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0E8red_buf1[4096];

	ld.param.u64 	%rd11, [Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_0];
	ld.param.u64 	%rd12, [Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_1];
	ld.param.u64 	%rd13, [Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_2];
	ld.param.u64 	%rd14, [Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_3];
	ld.param.u64 	%rd10, [Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_4];
	ld.param.u64 	%rd15, [Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0_param_5];
	cvta.to.global.u64 	%rd16, %rd15;
	mov.u32 	%r8, %ctaid.y;
	cvt.u64.u32	%rd1, %r8;
	mul.wide.u32 	%rd17, %r8, 4096;
	mov.u32 	%r1, %tid.y;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd18, %r1, 512;
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32	%rd3, %r2;
	add.s64 	%rd19, %rd17, %rd18;
	add.s64 	%rd20, %rd19, %rd3;
	cvta.to.global.u64 	%rd21, %rd13;
	shl.b64 	%rd22, %rd20, 1;
	add.s64 	%rd4, %rd21, %rd22;
	ld.global.nc.u16 	%rs2, [%rd4];
	cvta.to.global.u64 	%rd23, %rd11;
	mul.wide.u32 	%rd24, %r2, 2;
	add.s64 	%rd5, %rd23, %rd24;
	ld.global.nc.u16 	%rs3, [%rd5];
	// inline asm
	{add.f16 %rs1,%rs2,%rs3;
}
	// inline asm
	cvta.to.global.u64 	%rd25, %rd12;
	add.s64 	%rd6, %rd25, %rd22;
	ld.global.nc.u16 	%rs6, [%rd6];
	// inline asm
	{add.f16 %rs4,%rs1,%rs6;
}
	// inline asm
	add.s64 	%rd7, %rd16, %rd22;
	st.global.u16 	[%rd7], %rs4;
	// inline asm
	{  cvt.f32.f16 %f2, %rs4;}

	// inline asm
	cvta.to.global.u64 	%rd26, %rd14;
	shl.b64 	%rd27, %rd20, 2;
	add.s64 	%rd8, %rd26, %rd27;
	st.global.f32 	[%rd8], %f2;
	shl.b32 	%r9, %r1, 2;
	mov.u32 	%r10, _ZZ67Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0E53T_cast_T_add_T_add_input_0_input_1_input_3_red_shared;
	add.s32 	%r3, %r10, %r9;
	setp.ne.s32	%p2, %r2, 0;
	@%p2 bra 	BB0_2;

	mov.u32 	%r11, 0;
	st.shared.u32 	[%r3], %r11;

BB0_2:
	ld.global.nc.u16 	%rs9, [%rd4+256];
	ld.global.nc.u16 	%rs10, [%rd5+256];
	// inline asm
	{add.f16 %rs8,%rs9,%rs10;
}
	// inline asm
	ld.global.nc.u16 	%rs13, [%rd6+256];
	// inline asm
	{add.f16 %rs11,%rs8,%rs13;
}
	// inline asm
	st.global.u16 	[%rd7+256], %rs11;
	// inline asm
	{  cvt.f32.f16 %f3, %rs11;}

	// inline asm
	st.global.f32 	[%rd8+512], %f3;
	ld.global.nc.u16 	%rs16, [%rd4+512];
	ld.global.nc.u16 	%rs17, [%rd5+512];
	// inline asm
	{add.f16 %rs15,%rs16,%rs17;
}
	// inline asm
	ld.global.nc.u16 	%rs20, [%rd6+512];
	// inline asm
	{add.f16 %rs18,%rs15,%rs20;
}
	// inline asm
	st.global.u16 	[%rd7+512], %rs18;
	// inline asm
	{  cvt.f32.f16 %f4, %rs18;}

	// inline asm
	st.global.f32 	[%rd8+1024], %f4;
	ld.global.nc.u16 	%rs23, [%rd4+768];
	ld.global.nc.u16 	%rs24, [%rd5+768];
	// inline asm
	{add.f16 %rs22,%rs23,%rs24;
}
	// inline asm
	ld.global.nc.u16 	%rs27, [%rd6+768];
	// inline asm
	{add.f16 %rs25,%rs22,%rs27;
}
	// inline asm
	st.global.u16 	[%rd7+768], %rs25;
	// inline asm
	{  cvt.f32.f16 %f5, %rs25;}

	// inline asm
	st.global.f32 	[%rd8+1536], %f5;
	bar.sync 	0;
	ld.global.f32 	%f6, [%rd8];
	add.f32 	%f7, %f6, 0f00000000;
	sub.f32 	%f8, %f7, %f6;
	ld.global.f32 	%f9, [%rd8+512];
	sub.f32 	%f10, %f9, %f8;
	add.f32 	%f11, %f7, %f10;
	sub.f32 	%f12, %f11, %f7;
	sub.f32 	%f13, %f12, %f10;
	ld.global.f32 	%f14, [%rd8+1024];
	sub.f32 	%f15, %f14, %f13;
	add.f32 	%f16, %f11, %f15;
	sub.f32 	%f17, %f16, %f11;
	sub.f32 	%f18, %f17, %f15;
	ld.global.f32 	%f19, [%rd8+1536];
	sub.f32 	%f20, %f19, %f18;
	add.f32 	%f21, %f16, %f20;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r13, %r12, %r1, %r2;
	and.b32  	%r4, %r13, 127;
	and.b32  	%r5, %r13, -128;
	add.s32 	%r14, %r5, %r4;
	shl.b32 	%r15, %r14, 2;
	mov.u32 	%r16, _ZZ67Fused_Add_Add_Cast_ReduceSum_Mul_split_15243217562986013938_kernel0E8red_buf1;
	add.s32 	%r6, %r16, %r15;
	st.shared.f32 	[%r6], %f21;
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB0_4;

	ld.shared.f32 	%f22, [%r6];
	ld.shared.f32 	%f23, [%r6+256];
	add.f32 	%f24, %f22, %f23;
	st.shared.f32 	[%r6], %f24;

BB0_4:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB0_6;

	ld.shared.f32 	%f25, [%r6];
	ld.shared.f32 	%f26, [%r6+128];
	add.f32 	%f27, %f25, %f26;
	st.shared.f32 	[%r6], %f27;

BB0_6:
	setp.lt.u32	%p1, %r4, 32;
	bar.sync 	0;
	@!%p1 bra 	BB0_9;
	bra.uni 	BB0_7;

BB0_7:
	ld.shared.f32 	%f28, [%r6];
	mov.b32 	 %r17, %f28;
	mov.u32 	%r18, 2;
	mov.u32 	%r19, 31;
	mov.u32 	%r20, 16;
	mov.u32 	%r21, -1;
	shfl.sync.down.b32 	%r22|%p5, %r17, %r20, %r19, %r21;
	mov.b32 	 %f29, %r22;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	 %r23, %f30;
	mov.u32 	%r24, 8;
	shfl.sync.down.b32 	%r25|%p6, %r23, %r24, %r19, %r21;
	mov.b32 	 %f31, %r25;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	 %r26, %f32;
	mov.u32 	%r27, 4;
	shfl.sync.down.b32 	%r28|%p7, %r26, %r27, %r19, %r21;
	mov.b32 	 %f33, %r28;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	 %r29, %f34;
	shfl.sync.down.b32 	%r30|%p8, %r29, %r18, %r19, %r21;
	mov.b32 	 %f35, %r30;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	 %r31, %f36;
	mov.u32 	%r32, 1;
	shfl.sync.down.b32 	%r33|%p9, %r31, %r32, %r19, %r21;
	mov.b32 	 %f37, %r33;
	add.f32 	%f1, %f36, %f37;
	setp.ne.s32	%p10, %r4, 0;
	@%p10 bra 	BB0_9;

	st.shared.f32 	[%r6], %f1;

BB0_9:
	bar.sync 	0;
	setp.ne.s32	%p11, %r4, 0;
	@%p11 bra 	BB0_11;

	shl.b32 	%r34, %r5, 2;
	add.s32 	%r36, %r16, %r34;
	ld.shared.f32 	%f38, [%r36];
	ld.shared.f32 	%f39, [%r3];
	add.f32 	%f40, %f39, %f38;
	st.shared.f32 	[%r3], %f40;

BB0_11:
	cvt.u32.u64	%r7, %rd3;
	bar.sync 	0;
	setp.ne.s32	%p12, %r7, 127;
	@%p12 bra 	BB0_13;

	ld.shared.f32 	%f41, [%r3];
	mul.f32 	%f42, %f41, 0f3B000000;
	shl.b64 	%rd28, %rd1, 3;
	add.s64 	%rd29, %rd28, %rd2;
	cvta.to.global.u64 	%rd30, %rd10;
	shl.b64 	%rd31, %rd29, 2;
	add.s64 	%rd32, %rd30, %rd31;
	st.global.f32 	[%rd32], %f42;

BB0_13:
	bar.sync 	0;
	ret;
}


